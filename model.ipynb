{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c284ffef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.7.5\n",
      "/d/GH/GitWorkSpace/bank_model_competiton\n",
      "0825_lgb_base_cv_v0.1.csv\n",
      "0825_lgb_base_cv_v0.2.csv\n",
      "0825_lgb_base_cv_v0.3.csv\n",
      "data_eda.py\n",
      "model.ipynb\n",
      "process.ipynb\n",
      "process.py\n",
      "test.dat\n",
      "testaa\n",
      "testaa.csv\n",
      "testaa_bank_statement.csv\n",
      "testaa_submit_example.csv\n",
      "train\n",
      "train.csv\n",
      "train.dat\n",
      "train_bank_statement.csv\n",
      "~$姣旇禌瀛楁�佃В閲�.xlsx\n",
      "鍒濊禌A姒滄暟鎹�闆�\n",
      "鏁版嵁鎺㈡煡.ipynb\n",
      "姣旇禌瀛楁�佃В閲�.xlsx\n"
     ]
    }
   ],
   "source": [
    "#coding=utf-8\n",
    "!python --version\n",
    "!pwd\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a5a7909e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from heamy.dataset import Dataset\n",
    "from heamy.estimator import Regressor, Classifier\n",
    "from heamy.pipeline import ModelsPipeline\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder,OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sys\n",
    "\n",
    "dummy_fea_str = \"id,title,career,zip_code,residence,syndicated,installment,level\"\n",
    "dummy_fea = dummy_fea_str.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "110a63d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xgb_feature(X_train, y_train, X_test, y_test=None):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective':'rank:pairwise',\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.02,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,#0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 1,  # 2 3\n",
    "              'seed': 1111,\n",
    "              'silent':1\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvali = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=800)\n",
    "    predict = model.predict(dvali)\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)\n",
    "\n",
    "def xgb_feature2(X_train, y_train, X_test, y_test=None):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective':'rank:pairwise',\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.015,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,#0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 1,  # 2 3\n",
    "              'seed': 11,\n",
    "              'silent':1\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvali = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=1200)\n",
    "    predict = model.predict(dvali)\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)\n",
    "\n",
    "def xgb_feature3(X_train, y_train, X_test, y_test=None):\n",
    "    # 模型参数\n",
    "    params = {'booster': 'gbtree',\n",
    "              'objective':'rank:pairwise',\n",
    "              'eval_metric' : 'auc',\n",
    "              'eta': 0.01,\n",
    "              'max_depth': 5,  # 4 3\n",
    "              'colsample_bytree': 0.7,#0.8\n",
    "              'subsample': 0.7,\n",
    "              'min_child_weight': 1,  # 2 3\n",
    "              'seed': 1,\n",
    "              'silent':1\n",
    "              }\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dvali = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, num_boost_round=2000)\n",
    "    predict = model.predict(dvali)\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)\n",
    "\n",
    "\n",
    "def et_model(X_train, y_train, X_test, y_test=None):\n",
    "    model = ExtraTreesClassifier(max_features = 'log2', n_estimators = 1000 , n_jobs = -1).fit(X_train,y_train)\n",
    "    return model.predict_proba(X_test)[:,1]\n",
    "\n",
    "def gbdt_model(X_train, y_train, X_test, y_test=None):\n",
    "    model = GradientBoostingClassifier(learning_rate = 0.02, max_features = 0.7, n_estimators = 700 , max_depth = 5).fit(X_train,y_train)\n",
    "    predict = model.predict_proba(X_test)[:,1]\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)\n",
    "\n",
    "def logistic_model(X_train, y_train, X_test, y_test=None):\n",
    "    model = LogisticRegression(penalty = 'l2').fit(X_train,y_train)\n",
    "    return model.predict_proba(X_test)[:,1]\n",
    "\n",
    "def lgb_feature(X_train, y_train, X_test, y_test=None):\n",
    "    lgb_train = lgb.Dataset(X_train, y_train,categorical_feature={'sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound','account_grade','industry'})\n",
    "    lgb_test = lgb.Dataset(X_test,categorical_feature={'sex', 'merriage', 'income', 'qq_bound', 'degree', 'wechat_bound','account_grade','industry'})\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric':'auc',\n",
    "        'num_leaves': 25,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.7,\n",
    "        'bagging_fraction': 0.7,\n",
    "        'bagging_freq': 5,\n",
    "        'min_data_in_leaf':5,\n",
    "        'max_bin':200,\n",
    "        'verbose': 0,\n",
    "    }\n",
    "    gbm = lgb.train(params,lgb_train,num_boost_round=2000)\n",
    "    predict = gbm.predict(X_test)\n",
    "    minmin = min(predict)\n",
    "    maxmax = max(predict)\n",
    "    vfunc = np.vectorize(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "    return vfunc(predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f8d1827",
   "metadata": {},
   "source": [
    "## 验证v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "379adf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'loan', 'term', 'interest_rate', 'issue_time',\n",
      "       'record_time', 'history_time', 'total_accounts', 'balance_accounts',\n",
      "       'balance_limit', 'balance', 'label', '0', '1', '2', '3', '4', '5', '6',\n",
      "       '7', 'level_A0', 'level_A1', 'level_A2', 'level_A3', 'level_A4',\n",
      "       'level_A5', 'level_B0', 'level_B1', 'level_B2', 'level_B3', 'level_B4',\n",
      "       'level_B5', 'level_C1', 'level_C2', 'level_C3', 'level_C4', 'level_C5',\n",
      "       'level_D1', 'level_D2', 'level_D3', 'level_D4', 'level_D5', 'level_E1',\n",
      "       'level_E2', 'level_E3', 'level_E4', 'level_E5'],\n",
      "      dtype='object')\n",
      "valid auc 0.5244078016628604\n",
      "valid auc 0.6572872490143379\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dummy_fea_str = \"id,title,career,zip_code,residence,syndicated,installment,level\"\n",
    "dummy_fea = dummy_fea_str.split(',')\n",
    "\n",
    "sample_data = pd.read_csv('train.dat', engine = 'python');\n",
    "sample_data = sample_data.fillna(0)\n",
    "dummy_df = pd.get_dummies(sample_data.loc[:,dummy_fea])\n",
    "# print(dummy_df)\n",
    "\n",
    "sample_data_concat = pd.concat([sample_data, dummy_df], axis=1)\n",
    "sample_data_concat =sample_data_concat.fillna(0)\n",
    "vaild_sample_data = sample_data_concat.drop(dummy_fea, axis=1)\n",
    "sample_data_target = vaild_sample_data['label'].values\n",
    "print(vaild_sample_data.columns)\n",
    "sample_data_x = vaild_sample_data.drop(['label'],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_data_x, sample_data_target, test_size=0.2, random_state=None, shuffle=True)\n",
    "\n",
    "# logistic model\n",
    "predict_result = logistic_model(X_train, y_train, X_test, None)\n",
    "print('logistic_model valid auc', roc_auc_score(y_test, predict_result))\n",
    "\n",
    "# gbdt model\n",
    "predict_result = gbdt_model(X_train, y_train, X_test, None)\n",
    "print('gbdt_model valid auc', roc_auc_score(y_test, predict_result))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9358be",
   "metadata": {},
   "source": [
    "## 验证v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88571cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'loan', 'term', 'interest_rate', 'issue_time',\n",
      "       'record_time', 'history_time', 'total_accounts', 'balance_accounts',\n",
      "       'balance_limit', 'balance', 'label', '0', '1', '2', '3', '4', '5', '6',\n",
      "       '7', 'level_A0', 'level_A1', 'level_A2', 'level_A3', 'level_A4',\n",
      "       'level_A5', 'level_B0', 'level_B1', 'level_B2', 'level_B3', 'level_B4',\n",
      "       'level_B5', 'level_C1', 'level_C2', 'level_C3', 'level_C4', 'level_C5',\n",
      "       'level_D1', 'level_D2', 'level_D3', 'level_D4', 'level_D5', 'level_E1',\n",
      "       'level_E2', 'level_E3', 'level_E4', 'level_E5'],\n",
      "      dtype='object')\n",
      "[19:58:46] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:07] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:32] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[19:59:57] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:00:24] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:01:02] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:01:45] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:02:28] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:03:12] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:03:55] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:04:54] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:06:12] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:07:26] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:08:44] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "[20:09:55] WARNING: C:/Users/administrator/workspace/xgboost-win64_release_1.6.0/src/learner.cc:627: \n",
      "Parameters: { \"silent\" } might not be used.\n",
      "\n",
      "  This could be a false alarm, with some parameters getting used by language bindings but\n",
      "  then being mistakenly passed down to XGBoost core, or some parameter actually being used\n",
      "  but getting flagged wrongly here. Please open an issue if you find any such cases.\n",
      "\n",
      "\n",
      "ModelsPipeline(model_xgb, model_xgb2, model_xgb3, model_gbdt) valid auc 0.6542316376624142\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dummy_fea_str = \"id,title,career,zip_code,residence,syndicated,installment,level\"\n",
    "dummy_fea = dummy_fea_str.split(',')\n",
    "\n",
    "sample_data = pd.read_csv('train.dat', engine = 'python');\n",
    "sample_data = sample_data.fillna(0)\n",
    "dummy_df = pd.get_dummies(sample_data.loc[:,dummy_fea])\n",
    "print('dummy_df \\n',dummy_df)\n",
    "\n",
    "sample_data_concat = pd.concat([sample_data, dummy_df], axis=1)\n",
    "sample_data_concat =sample_data_concat.fillna(0)\n",
    "vaild_sample_data = sample_data_concat.drop(dummy_fea, axis=1)\n",
    "sample_data_target = vaild_sample_data['label'].values\n",
    "print(vaild_sample_data.columns)\n",
    "sample_data_x = vaild_sample_data.drop(['label'],axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sample_data_x, sample_data_target, test_size=0.2, random_state=None, shuffle=True)\n",
    "\n",
    "xgb_dataset = Dataset(X_train=X_train, y_train=y_train, X_test = X_test, y_test= None, use_cache=False)\n",
    "\n",
    "model_xgb  = Regressor(dataset=xgb_dataset, estimator=xgb_feature,name='xgb',use_cache=False)\n",
    "model_xgb2 = Regressor(dataset=xgb_dataset, estimator=xgb_feature2,name='xgb2',use_cache=False)\n",
    "model_xgb3 = Regressor(dataset=xgb_dataset, estimator=xgb_feature3,name='xgb3',use_cache=False)\n",
    "model_gbdt = Regressor(dataset=xgb_dataset, estimator=gbdt_model,name='gbdt',use_cache=False)\n",
    "\n",
    "pipeline = ModelsPipeline(model_xgb, model_xgb2, model_xgb3, model_gbdt)\n",
    "stack_ds = pipeline.stack(k=4, seed=111, add_diff=False, full_test=True)\n",
    "stacker = Regressor(dataset=stack_ds, estimator=LinearRegression, parameters={'fit_intercept': False})\n",
    "predict_result = stacker.predict()\n",
    "\n",
    "print('ModelsPipeline(model_xgb, model_xgb2, model_xgb3, model_gbdt) valid auc', roc_auc_score(y_test, predict_result))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57a661b",
   "metadata": {},
   "source": [
    "## lgb feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1b3c569b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id\n",
      "title\n",
      "career\n",
      "zip_code\n",
      "residence\n",
      "syndicated\n",
      "installment\n",
      "level\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\users\\chenchen\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "d:\\users\\chenchen\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv('train.dat',engine = 'python');\n",
    "train_data = train_data.fillna(0)\n",
    "\n",
    "test_data = pd.read_csv('test.dat',engine = 'python');\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "train_test_data = pd.concat([train_data, test_data],axis=0,ignore_index = True)\n",
    "train_test_data = train_test_data.fillna(0)\n",
    "\n",
    "train_data = train_test_data.iloc[:train_data.shape[0],:]\n",
    "test_data = train_test_data.iloc[train_data.shape[0]:,:]\n",
    "\n",
    "#处理dummy变量\n",
    "for _fea in dummy_fea:\n",
    "    print(_fea)\n",
    "    le = LabelEncoder()\n",
    "    le.fit(train_data[_fea].tolist() + test_data[_fea].tolist())\n",
    "    tmp = le.transform(train_data[_fea].tolist() + test_data[_fea].tolist())\n",
    "    train_data[_fea] = tmp[:train_data.shape[0]]\n",
    "    test_data[_fea]  = tmp[train_data.shape[0]:]\n",
    "    \n",
    "train_x = train_data.drop(['label'],axis=1)\n",
    "test_x = test_data.drop(['label'],axis=1)\n",
    "lgb_dataset = Dataset(train_x, train_data['label'], test_x, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337fe88f",
   "metadata": {},
   "source": [
    "## xgb feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5101a1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.dat',engine = 'python');\n",
    "train_data = train_data.fillna(0)\n",
    "\n",
    "test_data = pd.read_csv('test.dat',engine = 'python');\n",
    "test_data = test_data.fillna(0)\n",
    "\n",
    "train_test_data = pd.concat([train_data, test_data], axis=0, ignore_index = True)\n",
    "train_test_data = train_test_data.fillna(0)\n",
    "\n",
    "dummy_df = pd.get_dummies(train_test_data.loc[:,dummy_fea])\n",
    "train_test_data = pd.concat([train_test_data, dummy_df],axis=1)\n",
    "train_test_data = train_test_data.drop(dummy_fea, axis=1)\n",
    "\n",
    "train_train = train_test_data.iloc[:train_data.shape[0],:]\n",
    "test_test = train_test_data.iloc[train_data.shape[0]:,:]\n",
    "\n",
    "train_train_x = train_train.drop(['target'],axis=1)\n",
    "test_test_x = test_test.drop(['target'],axis=1)\n",
    "\n",
    "xgb_dataset = Dataset(X_train=train_train_x, y_train=train_train['target'], X_test = test_test_x, y_test= None, use_cache=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f6906bb",
   "metadata": {},
   "source": [
    "## stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66d276c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgb = Regressor(dataset=xgb_dataset, estimator=xgb_feature,name='xgb',use_cache=False)\n",
    "model_xgb2 = Regressor(dataset=xgb_dataset, estimator=xgb_feature2,name='xgb2',use_cache=False)\n",
    "model_xgb3 = Regressor(dataset=xgb_dataset, estimator=xgb_feature3,name='xgb3',use_cache=False)\n",
    "model_lgb = Regressor(dataset=lgb_dataset, estimator=lgb_feature,name='lgb',use_cache=False)\n",
    "model_gbdt = Regressor(dataset=xgb_dataset, estimator=gbdt_model,name='gbdt',use_cache=False)\n",
    "pipeline = ModelsPipeline(model_xgb, model_xgb2, model_xgb3, model_gbdt)\n",
    "stack_ds = pipeline.stack(k=5, seed=111, add_diff=False, full_test=True)\n",
    "stacker = Regressor(dataset=stack_ds, estimator=LinearRegression,parameters={'fit_intercept': False})\n",
    "predict_result = stacker.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e36d112",
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = pd.read_csv('testaa.csv')\n",
    "ans['PROB'] = predict_result\n",
    "minmin = min(ans['PROB']),\n",
    "maxmax = max(ans['PROB'])\n",
    "ans['PROB'] = ans['PROB'].map(lambda x:(x-minmin)/(maxmax-minmin))\n",
    "ans['PROB'] = ans['PROB'].map(lambda x:'%.4f' % x)\n",
    "ans.to_csv('./ans_stacking.csv',index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
