{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2a5c6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/d/GH/GitWorkSpace/bank_model_competiton\n",
      "202508261937_gh_v1.csv\n",
      "202508261937_gh_v2.csv\n",
      "model_stacking_v5.ipynb\n",
      "process_v5.ipynb\n",
      "test.dat.202508261937\n",
      "train.dat.202508261937\n"
     ]
    }
   ],
   "source": [
    "!pwd\n",
    "!ls ./data/202508261937\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3947a6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class GradientBoostingClassifier in module sklearn.ensemble._gb:\n",
      "\n",
      "class GradientBoostingClassifier(sklearn.base.ClassifierMixin, BaseGradientBoosting)\n",
      " |  GradientBoostingClassifier(*, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |  \n",
      " |  Gradient Boosting for classification.\n",
      " |  \n",
      " |  GB builds an additive model in a\n",
      " |  forward stage-wise fashion; it allows for the optimization of\n",
      " |  arbitrary differentiable loss functions. In each stage ``n_classes_``\n",
      " |  regression trees are fit on the negative gradient of the\n",
      " |  binomial or multinomial deviance loss function. Binary classification\n",
      " |  is a special case where only a single regression tree is induced.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <gradient_boosting>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  loss : {'deviance', 'exponential'}, default='deviance'\n",
      " |      The loss function to be optimized. 'deviance' refers to\n",
      " |      deviance (= logistic regression) for classification\n",
      " |      with probabilistic outputs. For loss 'exponential' gradient\n",
      " |      boosting recovers the AdaBoost algorithm.\n",
      " |  \n",
      " |  learning_rate : float, default=0.1\n",
      " |      Learning rate shrinks the contribution of each tree by `learning_rate`.\n",
      " |      There is a trade-off between learning_rate and n_estimators.\n",
      " |  \n",
      " |  n_estimators : int, default=100\n",
      " |      The number of boosting stages to perform. Gradient boosting\n",
      " |      is fairly robust to over-fitting so a large number usually\n",
      " |      results in better performance.\n",
      " |  \n",
      " |  subsample : float, default=1.0\n",
      " |      The fraction of samples to be used for fitting the individual base\n",
      " |      learners. If smaller than 1.0 this results in Stochastic Gradient\n",
      " |      Boosting. `subsample` interacts with the parameter `n_estimators`.\n",
      " |      Choosing `subsample < 1.0` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |  criterion : {'friedman_mse', 'squared_error', 'mse', 'mae'},             default='friedman_mse'\n",
      " |      The function to measure the quality of a split. Supported criteria\n",
      " |      are 'friedman_mse' for the mean squared error with improvement\n",
      " |      score by Friedman, 'squared_error' for mean squared error, and 'mae'\n",
      " |      for the mean absolute error. The default value of 'friedman_mse' is\n",
      " |      generally the best as it can provide a better approximation in some\n",
      " |      cases.\n",
      " |  \n",
      " |      .. versionadded:: 0.18\n",
      " |  \n",
      " |      .. deprecated:: 0.24\n",
      " |          `criterion='mae'` is deprecated and will be removed in version\n",
      " |          1.1 (renaming of 0.26). Use `criterion='friedman_mse'` or\n",
      " |          `'squared_error'` instead, as trees should use a squared error\n",
      " |          criterion in Gradient Boosting.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Criterion 'mse' was deprecated in v1.0 and will be removed in\n",
      " |          version 1.2. Use `criterion='squared_error'` which is equivalent.\n",
      " |  \n",
      " |  min_samples_split : int or float, default=2\n",
      " |      The minimum number of samples required to split an internal node:\n",
      " |  \n",
      " |      - If int, then consider `min_samples_split` as the minimum number.\n",
      " |      - If float, then `min_samples_split` is a fraction and\n",
      " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
      " |        number of samples for each split.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_samples_leaf : int or float, default=1\n",
      " |      The minimum number of samples required to be at a leaf node.\n",
      " |      A split point at any depth will only be considered if it leaves at\n",
      " |      least ``min_samples_leaf`` training samples in each of the left and\n",
      " |      right branches.  This may have the effect of smoothing the model,\n",
      " |      especially in regression.\n",
      " |  \n",
      " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
      " |      - If float, then `min_samples_leaf` is a fraction and\n",
      " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      " |        number of samples for each node.\n",
      " |  \n",
      " |      .. versionchanged:: 0.18\n",
      " |         Added float values for fractions.\n",
      " |  \n",
      " |  min_weight_fraction_leaf : float, default=0.0\n",
      " |      The minimum weighted fraction of the sum total of weights (of all\n",
      " |      the input samples) required to be at a leaf node. Samples have\n",
      " |      equal weight when sample_weight is not provided.\n",
      " |  \n",
      " |  max_depth : int, default=3\n",
      " |      The maximum depth of the individual regression estimators. The maximum\n",
      " |      depth limits the number of nodes in the tree. Tune this parameter\n",
      " |      for best performance; the best value depends on the interaction\n",
      " |      of the input variables.\n",
      " |  \n",
      " |  min_impurity_decrease : float, default=0.0\n",
      " |      A node will be split if this split induces a decrease of the impurity\n",
      " |      greater than or equal to this value.\n",
      " |  \n",
      " |      The weighted impurity decrease equation is the following::\n",
      " |  \n",
      " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      " |                              - N_t_L / N_t * left_impurity)\n",
      " |  \n",
      " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
      " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
      " |  \n",
      " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      " |      if ``sample_weight`` is passed.\n",
      " |  \n",
      " |      .. versionadded:: 0.19\n",
      " |  \n",
      " |  init : estimator or 'zero', default=None\n",
      " |      An estimator object that is used to compute the initial predictions.\n",
      " |      ``init`` has to provide :meth:`fit` and :meth:`predict_proba`. If\n",
      " |      'zero', the initial raw predictions are set to zero. By default, a\n",
      " |      ``DummyEstimator`` predicting the classes priors is used.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, default=None\n",
      " |      Controls the random seed given to each Tree estimator at each\n",
      " |      boosting iteration.\n",
      " |      In addition, it controls the random permutation of the features at\n",
      " |      each split (see Notes for more details).\n",
      " |      It also controls the random splitting of the training data to obtain a\n",
      " |      validation set if `n_iter_no_change` is not None.\n",
      " |      Pass an int for reproducible output across multiple function calls.\n",
      " |      See :term:`Glossary <random_state>`.\n",
      " |  \n",
      " |  max_features : {'auto', 'sqrt', 'log2'}, int or float, default=None\n",
      " |      The number of features to consider when looking for the best split:\n",
      " |  \n",
      " |      - If int, then consider `max_features` features at each split.\n",
      " |      - If float, then `max_features` is a fraction and\n",
      " |        `int(max_features * n_features)` features are considered at each\n",
      " |        split.\n",
      " |      - If 'auto', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'sqrt', then `max_features=sqrt(n_features)`.\n",
      " |      - If 'log2', then `max_features=log2(n_features)`.\n",
      " |      - If None, then `max_features=n_features`.\n",
      " |  \n",
      " |      Choosing `max_features < n_features` leads to a reduction of variance\n",
      " |      and an increase in bias.\n",
      " |  \n",
      " |      Note: the search for a split does not stop until at least one\n",
      " |      valid partition of the node samples is found, even if it requires to\n",
      " |      effectively inspect more than ``max_features`` features.\n",
      " |  \n",
      " |  verbose : int, default=0\n",
      " |      Enable verbose output. If 1 then it prints progress and performance\n",
      " |      once in a while (the more trees the lower the frequency). If greater\n",
      " |      than 1 then it prints progress and performance for every tree.\n",
      " |  \n",
      " |  max_leaf_nodes : int, default=None\n",
      " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      " |      Best nodes are defined as relative reduction in impurity.\n",
      " |      If None then unlimited number of leaf nodes.\n",
      " |  \n",
      " |  warm_start : bool, default=False\n",
      " |      When set to ``True``, reuse the solution of the previous call to fit\n",
      " |      and add more estimators to the ensemble, otherwise, just erase the\n",
      " |      previous solution. See :term:`the Glossary <warm_start>`.\n",
      " |  \n",
      " |  validation_fraction : float, default=0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if ``n_iter_no_change`` is set to an integer.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  n_iter_no_change : int, default=None\n",
      " |      ``n_iter_no_change`` is used to decide if early stopping will be used\n",
      " |      to terminate training when validation score is not improving. By\n",
      " |      default it is set to None to disable early stopping. If set to a\n",
      " |      number, it will set aside ``validation_fraction`` size of the training\n",
      " |      data as validation and terminate training when validation score is not\n",
      " |      improving in all of the previous ``n_iter_no_change`` numbers of\n",
      " |      iterations. The split is stratified.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  tol : float, default=1e-4\n",
      " |      Tolerance for the early stopping. When the loss is not improving\n",
      " |      by at least tol for ``n_iter_no_change`` iterations (if set to a\n",
      " |      number), the training stops.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  ccp_alpha : non-negative float, default=0.0\n",
      " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      " |      subtree with the largest cost complexity that is smaller than\n",
      " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
      " |  \n",
      " |      .. versionadded:: 0.22\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  n_estimators_ : int\n",
      " |      The number of estimators as selected by early stopping (if\n",
      " |      ``n_iter_no_change`` is specified). Otherwise it is set to\n",
      " |      ``n_estimators``.\n",
      " |  \n",
      " |      .. versionadded:: 0.20\n",
      " |  \n",
      " |  feature_importances_ : ndarray of shape (n_features,)\n",
      " |      The impurity-based feature importances.\n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |  \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |  \n",
      " |  oob_improvement_ : ndarray of shape (n_estimators,)\n",
      " |      The improvement in loss (= deviance) on the out-of-bag samples\n",
      " |      relative to the previous iteration.\n",
      " |      ``oob_improvement_[0]`` is the improvement in\n",
      " |      loss of the first stage over the ``init`` estimator.\n",
      " |      Only available if ``subsample < 1.0``\n",
      " |  \n",
      " |  train_score_ : ndarray of shape (n_estimators,)\n",
      " |      The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n",
      " |      model at iteration ``i`` on the in-bag sample.\n",
      " |      If ``subsample == 1`` this is the deviance on the training data.\n",
      " |  \n",
      " |  loss_ : LossFunction\n",
      " |      The concrete ``LossFunction`` object.\n",
      " |  \n",
      " |  init_ : estimator\n",
      " |      The estimator that provides the initial predictions.\n",
      " |      Set via the ``init`` argument or ``loss.init_estimator``.\n",
      " |  \n",
      " |  estimators_ : ndarray of DecisionTreeRegressor of             shape (n_estimators, ``loss_.K``)\n",
      " |      The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary\n",
      " |      classification, otherwise n_classes.\n",
      " |  \n",
      " |  classes_ : ndarray of shape (n_classes,)\n",
      " |      The classes labels.\n",
      " |  \n",
      " |  n_features_ : int\n",
      " |      The number of data features.\n",
      " |  \n",
      " |      .. deprecated:: 1.0\n",
      " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
      " |          removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  n_features_in_ : int\n",
      " |      Number of features seen during :term:`fit`.\n",
      " |  \n",
      " |      .. versionadded:: 0.24\n",
      " |  \n",
      " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
      " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
      " |      has feature names that are all strings.\n",
      " |  \n",
      " |      .. versionadded:: 1.0\n",
      " |  \n",
      " |  n_classes_ : int\n",
      " |      The number of classes.\n",
      " |  \n",
      " |  max_features_ : int\n",
      " |      The inferred value of max_features.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  HistGradientBoostingClassifier : Histogram-based Gradient Boosting\n",
      " |      Classification Tree.\n",
      " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
      " |  RandomForestClassifier : A meta-estimator that fits a number of decision\n",
      " |      tree classifiers on various sub-samples of the dataset and uses\n",
      " |      averaging to improve the predictive accuracy and control over-fitting.\n",
      " |  AdaBoostClassifier : A meta-estimator that begins by fitting a classifier\n",
      " |      on the original dataset and then fits additional copies of the\n",
      " |      classifier on the same dataset where the weights of incorrectly\n",
      " |      classified instances are adjusted such that subsequent classifiers\n",
      " |      focus more on difficult cases.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The features are always randomly permuted at each split. Therefore,\n",
      " |  the best found split may vary, even with the same training data and\n",
      " |  ``max_features=n_features``, if the improvement of the criterion is\n",
      " |  identical for several splits enumerated during the search of the best\n",
      " |  split. To obtain a deterministic behaviour during fitting,\n",
      " |  ``random_state`` has to be fixed.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  J. Friedman, Greedy Function Approximation: A Gradient Boosting\n",
      " |  Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n",
      " |  \n",
      " |  J. Friedman, Stochastic Gradient Boosting, 1999\n",
      " |  \n",
      " |  T. Hastie, R. Tibshirani and J. Friedman.\n",
      " |  Elements of Statistical Learning Ed. 2, Springer, 2009.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  The following example shows how to fit a gradient boosting classifier with\n",
      " |  100 decision stumps as weak learners.\n",
      " |  \n",
      " |  >>> from sklearn.datasets import make_hastie_10_2\n",
      " |  >>> from sklearn.ensemble import GradientBoostingClassifier\n",
      " |  \n",
      " |  >>> X, y = make_hastie_10_2(random_state=0)\n",
      " |  >>> X_train, X_test = X[:2000], X[2000:]\n",
      " |  >>> y_train, y_test = y[:2000], y[2000:]\n",
      " |  \n",
      " |  >>> clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
      " |  ...     max_depth=1, random_state=0).fit(X_train, y_train)\n",
      " |  >>> clf.score(X_test, y_test)\n",
      " |  0.913...\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      GradientBoostingClassifier\n",
      " |      sklearn.base.ClassifierMixin\n",
      " |      BaseGradientBoosting\n",
      " |      sklearn.ensemble._base.BaseEnsemble\n",
      " |      sklearn.base.MetaEstimatorMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, loss='deviance', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=3, min_impurity_decrease=0.0, init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, validation_fraction=0.1, n_iter_no_change=None, tol=0.0001, ccp_alpha=0.0)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  decision_function(self, X)\n",
      " |      Compute the decision function of ``X``.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : ndarray of shape (n_samples, n_classes) or (n_samples,)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          order of the classes corresponds to that in the attribute\n",
      " |          :term:`classes_`. Regression and binary classification produce an\n",
      " |          array of shape (n_samples,).\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict class for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : ndarray of shape (n_samples,)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  predict_log_proba(self, X)\n",
      " |      Predict class log-probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class log-probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |  \n",
      " |  predict_proba(self, X)\n",
      " |      Predict class probabilities for X.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      p : ndarray of shape (n_samples, n_classes)\n",
      " |          The class probabilities of the input samples. The order of the\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |      \n",
      " |      Raises\n",
      " |      ------\n",
      " |      AttributeError\n",
      " |          If the ``loss`` does not support probabilities.\n",
      " |  \n",
      " |  staged_decision_function(self, X)\n",
      " |      Compute decision function of ``X`` for each iteration.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      score : generator of ndarray of shape (n_samples, k)\n",
      " |          The decision function of the input samples, which corresponds to\n",
      " |          the raw values predicted from the trees of the ensemble . The\n",
      " |          classes corresponds to that in the attribute :term:`classes_`.\n",
      " |          Regression and binary classification are special cases with\n",
      " |          ``k == 1``, otherwise ``k==n_classes``.\n",
      " |  \n",
      " |  staged_predict(self, X)\n",
      " |      Predict class at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Yields\n",
      " |      -------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  staged_predict_proba(self, X)\n",
      " |      Predict class probabilities at each stage for X.\n",
      " |      \n",
      " |      This method allows monitoring (i.e. determine error on testing set)\n",
      " |      after each stage.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Yields\n",
      " |      ------\n",
      " |      y : generator of ndarray of shape (n_samples,)\n",
      " |          The predicted value of the input samples.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the mean accuracy on the given test data and labels.\n",
      " |      \n",
      " |      In multi-label classification, this is the subset accuracy\n",
      " |      which is a harsh metric since you require for each sample that\n",
      " |      each label set be correctly predicted.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True labels for `X`.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  apply(self, X)\n",
      " |      Apply trees in the ensemble to X, return leaf indices.\n",
      " |      \n",
      " |      .. versionadded:: 0.17\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, its dtype will be converted to\n",
      " |          ``dtype=np.float32``. If a sparse matrix is provided, it will\n",
      " |          be converted to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_leaves : array-like of shape (n_samples, n_estimators, n_classes)\n",
      " |          For each datapoint x in X and for each tree in the ensemble,\n",
      " |          return the index of the leaf x ends up in each estimator.\n",
      " |          In the case of binary classification n_classes is 1.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None, monitor=None)\n",
      " |      Fit the gradient boosting model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          The input samples. Internally, it will be converted to\n",
      " |          ``dtype=np.float32`` and if a sparse matrix is provided\n",
      " |          to a sparse ``csr_matrix``.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target values (strings or integers in classification, real numbers\n",
      " |          in regression)\n",
      " |          For classification, labels must correspond to classes.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights. If None, then samples are equally weighted. Splits\n",
      " |          that would create child nodes with net zero or negative weight are\n",
      " |          ignored while searching for a split in each node. In the case of\n",
      " |          classification, splits are also ignored if they would result in any\n",
      " |          single class carrying a negative weight in either child node.\n",
      " |      \n",
      " |      monitor : callable, default=None\n",
      " |          The monitor is called after each iteration with the current\n",
      " |          iteration, a reference to the estimator and the local variables of\n",
      " |          ``_fit_stages`` as keyword arguments ``callable(i, self,\n",
      " |          locals())``. If the callable returns ``True`` the fitting procedure\n",
      " |          is stopped. The monitor can be used for various things such as\n",
      " |          computing held-out estimates, early stopping, model introspect, and\n",
      " |          snapshoting.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Fitted estimator.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseGradientBoosting:\n",
      " |  \n",
      " |  feature_importances_\n",
      " |      The impurity-based feature importances.\n",
      " |      \n",
      " |      The higher, the more important the feature.\n",
      " |      The importance of a feature is computed as the (normalized)\n",
      " |      total reduction of the criterion brought by that feature.  It is also\n",
      " |      known as the Gini importance.\n",
      " |      \n",
      " |      Warning: impurity-based feature importances can be misleading for\n",
      " |      high cardinality features (many unique values). See\n",
      " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      feature_importances_ : ndarray of shape (n_features,)\n",
      " |          The values of this array sum to 1, unless all trees are single node\n",
      " |          trees consisting of only the root node, in which case it will be an\n",
      " |          array of zeros.\n",
      " |  \n",
      " |  n_features_\n",
      " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __getitem__(self, index)\n",
      " |      Return the index'th estimator in the ensemble.\n",
      " |  \n",
      " |  __iter__(self)\n",
      " |      Return iterator over estimators in the ensemble.\n",
      " |  \n",
      " |  __len__(self)\n",
      " |      Return the number of estimators in the ensemble.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
      " |  \n",
      " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "help(GradientBoostingClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bfb7907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   a_a  a_b  a_c  b_3  b_4  b_5\n",
      "0    1    0    0    1    0    0\n",
      "1    0    1    0    0    1    0\n",
      "2    0    0    1    0    0    1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'a':['a', 'b', 'c'], 'b':['3','4','5']} )\n",
    "print(pd.get_dummies(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "305df1c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20054.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.304028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.266113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.068901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.191038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.531130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  20054.000000\n",
       "mean       0.304028\n",
       "std        0.266113\n",
       "min        0.000000\n",
       "25%        0.068901\n",
       "50%        0.191038\n",
       "75%        0.531130\n",
       "max        1.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/202508261937/202508261937_gh_v1.csv')\n",
    "df[['label']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d8ac518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20054.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.209139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.117666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.143666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.199733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.234548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              label\n",
       "count  20054.000000\n",
       "mean       0.209139\n",
       "std        0.117666\n",
       "min        0.000000\n",
       "25%        0.143666\n",
       "50%        0.199733\n",
       "75%        0.234548\n",
       "max        1.000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/202508261937/202508261937_gh_v2.csv')\n",
    "df[['label']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c324413f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "序数编码结果:\n",
      " [[ 0.]\n",
      " [ 6.]\n",
      " [ 1.]\n",
      " [21.]]\n",
      "解码后的类别:\n",
      " [['A1']\n",
      " ['B2']\n",
      " ['A2']\n",
      " ['E2']]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([[\"A1\"], [\"B2\"], [\"A2\"], [\"E2\"]])\n",
    "encoder = OrdinalEncoder(categories=[['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5','C1','C2','C3','C4','C5','D1','D2','D3','D4','D5','E1','E2','E3','E4','E5',]]) \n",
    "\n",
    "\n",
    "X_encoded = encoder.fit_transform(X)\n",
    "print(\"序数编码结果:\\n\", X_encoded)\n",
    "\n",
    "# 解码回原始类别\n",
    "decoded_labels = encoder.inverse_transform(X_encoded)\n",
    "print(\"解码后的类别:\\n\", decoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7dc1dee4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 1.],\n",
       "       [ 2.],\n",
       "       [ 3.],\n",
       "       [ 4.],\n",
       "       [ 5.],\n",
       "       [ 6.],\n",
       "       [ 7.],\n",
       "       [ 8.],\n",
       "       [ 9.],\n",
       "       [10.],\n",
       "       [11.],\n",
       "       [12.],\n",
       "       [13.],\n",
       "       [14.],\n",
       "       [15.],\n",
       "       [16.],\n",
       "       [17.],\n",
       "       [18.],\n",
       "       [19.],\n",
       "       [20.],\n",
       "       [21.],\n",
       "       [22.],\n",
       "       [23.],\n",
       "       [24.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "\n",
    "# 创建示例数据\n",
    "data = {'level': ['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5','C1','C2','C3','C4','C5','D1','D2','D3','D4','D5','E1','E2','E3','E4','E5']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 初始化OrdinalEncoder\n",
    "# enc = OrdinalEncoder()\n",
    "# enc.fit(df)\n",
    "enc = OrdinalEncoder(categories=[['A1','A2','A3','A4','A5','B1','B2','B3','B4','B5','C1','C2','C3','C4','C5','D1','D2','D3','D4','D5','E1','E2','E3','E4','E5',]]) \n",
    "\n",
    "\n",
    "# 对有序分类变量进行编码\n",
    "df_encoded = enc.fit_transform(df[['level']])\n",
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "988e8602",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(163, 163)</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(642, 642)</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(638, 638)</td>\n",
       "      <td>638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(264, 264)</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(536, 536)</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>(320, 320)</td>\n",
       "      <td>320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>(527, 527)</td>\n",
       "      <td>527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>(996, 996)</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>(125, 125)</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>(265, 265)</td>\n",
       "      <td>265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              0    1\n",
       "0    (163, 163)  163\n",
       "1    (642, 642)  642\n",
       "2    (638, 638)  638\n",
       "3    (264, 264)  264\n",
       "4    (536, 536)  536\n",
       "..          ...  ...\n",
       "745  (320, 320)  320\n",
       "746  (527, 527)  527\n",
       "747  (996, 996)  996\n",
       "748  (125, 125)  125\n",
       "749  (265, 265)  265\n",
       "\n",
       "[750 rows x 2 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_rows', 100) \n",
    "arr = [(i,i)for i in  range(0,1000)]\n",
    "target = [i for i in  range(0,1000)]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(arr , target ,random_state = 10)\n",
    "pd.DataFrame([X_train, y_train]).T\n",
    "# X_train\n",
    "# y_train\n",
    "# help(train_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8ee8ab00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='label'>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_pred = pd.read_csv('./0828_gbdt_v3.csv')\n",
    "df_ret = df_pred.groupby(pd.cut(df_pred['label'], bins=20))['label'].agg('count')\n",
    "df_ret.plot(kind='bar')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
